{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOJSvSMxos5X",
        "outputId": "d61ae88a-6189-4b3d-9005-3623ff18d775"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nvcc4jupyter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uaJWJBaDoyWy",
        "outputId": "86ca862e-5d8c-4c4a-f1a5-d760c07ba39c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvcc4jupyter\n",
            "  Downloading nvcc4jupyter-1.2.1-py3-none-any.whl.metadata (5.1 kB)\n",
            "Downloading nvcc4jupyter-1.2.1-py3-none-any.whl (10 kB)\n",
            "Installing collected packages: nvcc4jupyter\n",
            "Successfully installed nvcc4jupyter-1.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext nvcc4jupyter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ND793YmmpJRZ",
        "outputId": "4ec46105-db9f-4f5a-9507-526eb2f17089"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected platform \"Colab\". Running its setup...\n",
            "Source files will be saved in \"/tmp/tmpwiq9bkqx\".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPU convolution"
      ],
      "metadata": {
        "id": "CnDaIWk3Ga5C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda\n",
        "#include <stdio.h>\n",
        "#include <cuda.h>\n",
        "#include <cassert>\n",
        "#include <curand_kernel.h>\n",
        "#include <time.h>\n",
        "\n",
        "#define N_M 16000 // Dimensione della matrice input\n",
        "#define N_F 5 // Dimensione del filtro\n",
        "#define BLOCK_SIZE 32\n",
        "\n",
        "// Kernel CUDA per la convoluzione\n",
        "__global__ void gpu_matrix_convolution(int *input_matrix, int *mask, int *output_matrix, int n_m, int n_f) {\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    // Starting index for calculation\n",
        "    int start_r = row - n_f / 2;\n",
        "    int start_c = col - n_f / 2;\n",
        "\n",
        "    // Temp value for accumulating the result\n",
        "    int temp = 0;\n",
        "\n",
        "    // Iterate over all the rows\n",
        "    #pragma unroll\n",
        "    for (int i = 0; i < n_f; i++) {\n",
        "        // Go over each column\n",
        "        #pragma unroll\n",
        "        for (int j = 0; j < n_f; j++) {\n",
        "            // Range check for rows\n",
        "            if ((start_r + i) >= 0 && (start_r + i) < n_m) {\n",
        "                // Range check for columns\n",
        "                if ((start_c + j) >= 0 && (start_c + j) < n_m) {\n",
        "                    // Accumulate result\n",
        "                    temp += input_matrix[(start_r + i) * n_m + (start_c + j)] *\n",
        "                            mask[i * n_f + j];\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    if (row < n_m && col < n_m) {\n",
        "        // Write back the result\n",
        "        output_matrix[row * n_m + col] = temp;\n",
        "    }\n",
        "}\n",
        "\n",
        "void verify_result(int *input_matrix, int *mask, int *output_matrix, int n_m, int n_f) {\n",
        "    // Temp value for accumulating results\n",
        "    int temp;\n",
        "\n",
        "    // Intermediate value for more readable code\n",
        "    int offset_r;\n",
        "    int offset_c;\n",
        "\n",
        "    // Go over each row\n",
        "    for (int i = 0; i < n_m; i++) {\n",
        "        // Go over each column\n",
        "        for (int j = 0; j < n_m; j++) {\n",
        "            // Reset the temp variable\n",
        "            temp = 0;\n",
        "\n",
        "            // Go over each mask row\n",
        "            for (int k = 0; k < n_f; k++) {\n",
        "                // Update offset value for row\n",
        "                offset_r = i - (n_f / 2) + k;\n",
        "\n",
        "                // Go over each mask column\n",
        "                for (int l = 0; l < n_f; l++) {\n",
        "                    // Update offset value for column\n",
        "                    offset_c = j - (n_f / 2) + l;\n",
        "\n",
        "                    // Range checks if we are hanging off the matrix\n",
        "                    if (offset_r >= 0 && offset_r < n_m) {\n",
        "                        if (offset_c >= 0 && offset_c < n_m) {\n",
        "                            // Accumulate partial results\n",
        "                            temp += input_matrix[offset_r * n_m + offset_c] * mask[k * n_f + l];\n",
        "                        }\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "            // Fail if the results don't match\n",
        "            assert(output_matrix[i * n_m + j] == temp);\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    srand(time(NULL));\n",
        "\n",
        "    int block_size;\n",
        "\n",
        "    int *input_matrix, *mask, *output_matrix;\n",
        "    cudaMallocManaged((void **) &input_matrix, sizeof(int) * N_M * N_M);\n",
        "    cudaMallocManaged((void **) &mask, sizeof(int) * N_F * N_F);\n",
        "    cudaMallocManaged((void **) &output_matrix, sizeof(int) * N_M * N_M);\n",
        "\n",
        "    // Initialize input matrix\n",
        "    for (int i = 0; i < N_M; ++i) {\n",
        "        for (int j = 0; j < N_M; ++j) {\n",
        "            input_matrix[i * N_M + j] = rand() % 101;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // Initialize mask\n",
        "    for (int i = 0; i < N_F; i++) {\n",
        "        for (int j = 0; j < N_F; j++) {\n",
        "            mask[i * N_F + j] = rand() % 101;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    for (block_size = 32; block_size <= 32; block_size *= 2) {\n",
        "        float naive_gpu_elapsed_time_ms;\n",
        "\n",
        "        // Some events to count the execution time\n",
        "        cudaEvent_t start, stop;\n",
        "        cudaEventCreate(&start);\n",
        "        cudaEventCreate(&stop);\n",
        "\n",
        "        unsigned int grid_rows = (N_M + block_size - 1) / block_size;\n",
        "        unsigned int grid_cols = (N_M + block_size - 1) / block_size;\n",
        "        dim3 dimGrid(grid_cols, grid_rows);\n",
        "        dim3 dimBlock(block_size, block_size);\n",
        "\n",
        "        cudaEventRecord(start, 0);\n",
        "        gpu_matrix_convolution<<<dimGrid, dimBlock>>>(input_matrix, mask, output_matrix, N_M, N_F);\n",
        "        cudaThreadSynchronize();\n",
        "\n",
        "        // Time counting terminate\n",
        "        cudaEventRecord(stop, 0);\n",
        "        cudaEventSynchronize(stop);\n",
        "\n",
        "        // Compute time elapsed on GPU computing\n",
        "        cudaEventElapsedTime(&naive_gpu_elapsed_time_ms, start, stop);\n",
        "        printf(\"Time elapsed on naive GPU matrix convolution of %dx%d . %dx%d (%d): %f ms.\\n\\n\", N_M, N_M, N_M, N_M, block_size, naive_gpu_elapsed_time_ms);\n",
        "        verify_result(input_matrix, mask, output_matrix, N_M, N_F);\n",
        "        printf(\"success\\n\");\n",
        "    }\n",
        "\n",
        "    // Free memory\n",
        "    cudaFree(input_matrix);\n",
        "    cudaFree(mask);\n",
        "    cudaFree(output_matrix);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wc1gnm7cGdZP",
        "outputId": "fa8d7e6a-bdc0-4032-9cfe-afb189b51337"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time elapsed on naive GPU matrix convolution of 16000x16000 . 16000x16000 (32): 473.425873 ms.\n",
            "\n",
            "success\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPU tiled convolution"
      ],
      "metadata": {
        "id": "Aif93rSsHHqa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda\n",
        "#include <stdio.h>\n",
        "#include <cuda.h>\n",
        "#include <cassert>\n",
        "#include <curand_kernel.h>\n",
        "#include <time.h>\n",
        "\n",
        "#define N_M 16000 // Dimensione della matrice input\n",
        "#define N_F 5     // Dimensione del filtro\n",
        "#define TILE_WIDTH 8\n",
        "\n",
        "__global__ void gpu_matrix_convolution_tiling(int *input_matrix, int *mask, int *output_matrix, int n_m, int n_f) {\n",
        "    // Shared memory for the input matrix and filter\n",
        "    __shared__ int shared_a[TILE_WIDTH + N_F - 1][TILE_WIDTH + N_F - 1];\n",
        "\n",
        "    // Thread row and column within the block\n",
        "    int tx = threadIdx.x;\n",
        "    int ty = threadIdx.y;\n",
        "\n",
        "    // Global row and column indices\n",
        "    int row = blockIdx.y * TILE_WIDTH + ty;\n",
        "    int col = blockIdx.x * TILE_WIDTH + tx;\n",
        "\n",
        "    // Starting index for the convolution\n",
        "    int start_r = row - n_f / 2;\n",
        "    int start_c = col - n_f / 2;\n",
        "\n",
        "    // Load input matrix tile into shared memory with boundary checks\n",
        "    if (ty < TILE_WIDTH + N_F - 1 && tx < TILE_WIDTH + N_F - 1) {\n",
        "        if (start_r >= 0 && start_r < n_m && start_c >= 0 && start_c < n_m) {\n",
        "            shared_a[ty][tx] = input_matrix[start_r * n_m + start_c];\n",
        "        } else {\n",
        "            shared_a[ty][tx] = 0;\n",
        "        }\n",
        "    }\n",
        "\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "// Compute the convolution\n",
        "    if (ty < TILE_WIDTH && tx < TILE_WIDTH && row < n_m && col < n_m) {\n",
        "        int temp = 0;\n",
        "        #pragma unroll\n",
        "        for (int i = 0; i < n_f; i++) {\n",
        "            #pragma unroll\n",
        "            for (int j = 0; j < n_f; j++) {\n",
        "                temp += shared_a[ty + i][tx + j] * mask[i * n_f + j];\n",
        "            }\n",
        "        }\n",
        "        output_matrix[row * n_m + col] = temp;\n",
        "    }\n",
        "}\n",
        "\n",
        "void verify_result(int *input_matrix, int *mask, int *output_matrix, int n_m, int n_f) {\n",
        "    // Temp value for accumulating results\n",
        "    int temp;\n",
        "\n",
        "    // Intermediate value for more readable code\n",
        "    int offset_r;\n",
        "    int offset_c;\n",
        "\n",
        "    // Go over each row\n",
        "    for (int i = 0; i < n_m; i++) {\n",
        "        // Go over each column\n",
        "        for (int j = 0; j < n_m; j++) {\n",
        "            // Reset the temp variable\n",
        "            temp = 0;\n",
        "\n",
        "            // Go over each mask row\n",
        "            for (int k = 0; k < n_f; k++) {\n",
        "                offset_r = i - (n_f / 2) + k;\n",
        "\n",
        "                // Go over each mask column\n",
        "                for (int l = 0; l < n_f; l++) {\n",
        "                    offset_c = j - (n_f / 2) + l;\n",
        "\n",
        "                    // Range checks if we are hanging off the matrix\n",
        "                    if (offset_r >= 0 && offset_r < n_m && offset_c >= 0 && offset_c < n_m) {\n",
        "                        temp += input_matrix[offset_r * n_m + offset_c] * mask[k * n_f + l];\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "\n",
        "\n",
        "            assert(output_matrix[i * n_m + j] == temp);\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    srand(time(NULL));\n",
        "\n",
        "    int *input_matrix, *mask, *output_matrix;\n",
        "    cudaMallocManaged((void **) &input_matrix, sizeof(int) * N_M * N_M);\n",
        "    cudaMallocManaged((void **) &mask, sizeof(int) * N_F * N_F);\n",
        "    cudaMallocManaged((void **) &output_matrix, sizeof(int) * N_M * N_M);\n",
        "\n",
        "    // Initialize input matrix\n",
        "    for (int i = 0; i < N_M; ++i) {\n",
        "        for (int j = 0; j < N_M; ++j) {\n",
        "            input_matrix[i * N_M + j] = rand() % 101;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // Initialize mask\n",
        "    for (int i = 0; i < N_F; i++) {\n",
        "        for (int j = 0; j < N_F; j++) {\n",
        "            mask[i * N_F + j] = rand() % 101;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    cudaMemPrefetchAsync(input_matrix, sizeof(int) * N_M * N_M, 0);\n",
        "    cudaMemPrefetchAsync(mask, sizeof(int) * N_F * N_F, 0);\n",
        "    cudaMemPrefetchAsync(output_matrix, sizeof(int) * N_M * N_M, 0);\n",
        "\n",
        "    float gpu_elapsed_time_ms;\n",
        "\n",
        "    // Setup timing events\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "\n",
        "    dim3 dimGrid((N_M + TILE_WIDTH - 1) / TILE_WIDTH, (N_M + TILE_WIDTH - 1) / TILE_WIDTH);\n",
        "    dim3 dimBlock(TILE_WIDTH + N_F - 1, TILE_WIDTH + N_F - 1);\n",
        "\n",
        "    cudaEventRecord(start, 0);\n",
        "    gpu_matrix_convolution_tiling<<<dimGrid, dimBlock>>>(input_matrix, mask, output_matrix, N_M, N_F);\n",
        "    cudaThreadSynchronize();\n",
        "\n",
        "    cudaEventRecord(stop, 0);\n",
        "    cudaEventSynchronize(stop);\n",
        "\n",
        "    cudaEventElapsedTime(&gpu_elapsed_time_ms, start, stop);\n",
        "    printf(\"Time elapsed on GPU matrix convolution with tiling of %dx%d and filter %dx%d: %f ms.\\n\", N_M, N_M, N_F, N_F, gpu_elapsed_time_ms);\n",
        "\n",
        "    verify_result(input_matrix, mask, output_matrix, N_M, N_F);\n",
        "    printf(\"success\\n\");\n",
        "\n",
        "    cudaFree(input_matrix);\n",
        "    cudaFree(mask);\n",
        "    cudaFree(output_matrix);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTod6o5uHMCl",
        "outputId": "3ab27b9f-abf8-4f1e-ccbc-3dd4bfed768d"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time elapsed on GPU matrix convolution with tiling of 16000x16000 and filter 5x5: 221.876633 ms.\n",
            "Verification success!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPU Convolution with declared ___costant___ mask"
      ],
      "metadata": {
        "id": "sXhjeKSFoNf2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda\n",
        "#include <stdio.h>\n",
        "#include <cuda.h>\n",
        "#include <cassert>\n",
        "#include <curand_kernel.h>\n",
        "#include <time.h>\n",
        "\n",
        "#define N_M 16000 // Dimensione della matrice input\n",
        "#define N_F 5 // Dimensione del filtro\n",
        "#define BLOCK_SIZE 32\n",
        "\n",
        "__constant__ int mask[N_F * N_F];\n",
        "\n",
        "// Kernel CUDA per la convoluzione\n",
        "__global__ void gpu_matrix_convolution(int *input_matrix, int *output_matrix, int n_m, int n_f) {\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    // Starting index for calculation\n",
        "    int start_r = row - n_f / 2;\n",
        "    int start_c = col - n_f / 2;\n",
        "\n",
        "    // Temp value for accumulating the result\n",
        "    int temp = 0;\n",
        "\n",
        "    // Iterate over all the rows\n",
        "    #pragma unroll\n",
        "    for (int i = 0; i < n_f; i++) {\n",
        "        // Go over each column\n",
        "        #pragma unroll\n",
        "        for (int j = 0; j < n_f; j++) {\n",
        "            // Range check for rows\n",
        "            if ((start_r + i) >= 0 && (start_r + i) < n_m) {\n",
        "                // Range check for columns\n",
        "                if ((start_c + j) >= 0 && (start_c + j) < n_m) {\n",
        "                    // Accumulate result\n",
        "                    temp += input_matrix[(start_r + i) * n_m + (start_c + j)] *\n",
        "                            mask[i * n_f + j];\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    if (row < n_m && col < n_m) {\n",
        "        // Write back the result\n",
        "        output_matrix[row * n_m + col] = temp;\n",
        "    }\n",
        "}\n",
        "\n",
        "void verify_result(int *m, int *mask, int *result, int n_m, int n_f) {\n",
        "    int temp;\n",
        "    int offset_r;\n",
        "    int offset_c;\n",
        "\n",
        "    for (int i = 0; i < n_m; i++) {\n",
        "        for (int j = 0; j < n_m; j++) {\n",
        "            temp = 0;\n",
        "            for (int k = 0; k < n_f; k++) {\n",
        "                offset_r = i - (n_f / 2) + k;\n",
        "                for (int l = 0; l < n_f; l++) {\n",
        "                    offset_c = j - (n_f / 2) + l;\n",
        "                    if (offset_r >= 0 && offset_r < n_m) {\n",
        "                        if (offset_c >= 0 && offset_c < n_m) {\n",
        "                            temp += m[offset_r * n_m + offset_c] * mask[k * n_f + l];\n",
        "                        }\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "            assert(result[i * n_m + j] == temp);\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    srand(time(NULL));\n",
        "\n",
        "    int block_size;\n",
        "    int *input_matrix, *output_matrix;\n",
        "    cudaMallocManaged((void **)&input_matrix, sizeof(int) * N_M * N_M);\n",
        "    cudaMallocManaged((void **)&output_matrix, sizeof(int) * N_M * N_M);\n",
        "\n",
        "    // Initialize matrix input_matrix\n",
        "    for (int i = 0; i < N_M; ++i) {\n",
        "        for (int j = 0; j < N_M; ++j) {\n",
        "            input_matrix[i * N_M + j] = rand() % 101;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    int filter[N_F * N_F];\n",
        "    for (int i = 0; i < N_F * N_F; i++) {\n",
        "        filter[i] = rand() % 101;\n",
        "    }\n",
        "\n",
        "    cudaMemcpyToSymbol(mask, filter, sizeof(int) * N_F * N_F);\n",
        "    cudaMemPrefetchAsync(input_matrix, sizeof(int) * N_M * N_M, 0);\n",
        "    cudaMemPrefetchAsync(output_matrix, sizeof(int) * N_M * N_M, 0);\n",
        "\n",
        "    for (block_size = 32; block_size <= 32; block_size *= 2) {\n",
        "        float naive_gpu_elapsed_time_ms;\n",
        "\n",
        "        cudaEvent_t start, stop;\n",
        "        cudaEventCreate(&start);\n",
        "        cudaEventCreate(&stop);\n",
        "\n",
        "        unsigned int grid_rows = (N_M + block_size - 1) / block_size;\n",
        "        unsigned int grid_cols = (N_M + block_size - 1) / block_size;\n",
        "        dim3 dimGrid(grid_cols, grid_rows);\n",
        "        dim3 dimBlock(block_size, block_size);\n",
        "\n",
        "        cudaEventRecord(start, 0);\n",
        "        gpu_matrix_convolution<<<dimGrid, dimBlock>>>(input_matrix, output_matrix, N_M, N_F);\n",
        "        cudaThreadSynchronize();\n",
        "\n",
        "        cudaEventRecord(stop, 0);\n",
        "        cudaEventSynchronize(stop);\n",
        "\n",
        "        cudaEventElapsedTime(&naive_gpu_elapsed_time_ms, start, stop);\n",
        "        printf(\"Time elapsed on naive GPU matrix convolution of %dx%d . %dx%d (%d): %f ms.\\n\\n\", N_M, N_M, N_M, N_M, block_size, naive_gpu_elapsed_time_ms);\n",
        "\n",
        "        verify_result(input_matrix, filter, output_matrix, N_M, N_F);\n",
        "        printf(\"success\\n\");\n",
        "    }\n",
        "\n",
        "    cudaFree(input_matrix);\n",
        "    cudaFree(output_matrix);\n",
        "    return 0;\n",
        "}\n",
        "\n"
      ],
      "metadata": {
        "id": "patSEnDlUqdx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6f80ed5-6c3d-4ae3-918b-6cc2b68ec9cd"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time elapsed on naive GPU matrix convolution of 16000x16000 . 16000x16000 (32): 49.178623 ms.\n",
            "\n",
            "success\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "gpu tiled convolution with declared ___costant___ mask\n"
      ],
      "metadata": {
        "id": "Qjj_3MPNoWE9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda\n",
        "#include <stdio.h>\n",
        "#include <cuda.h>\n",
        "#include <cassert>\n",
        "#include <curand_kernel.h>\n",
        "#include <time.h>\n",
        "\n",
        "#define N_M 16000 // Dimensione della matrice input\n",
        "#define N_F 5     // Dimensione del filtro\n",
        "#define TILE_WIDTH 16 // Dimensione della tile ottimizzata per performance\n",
        "\n",
        "__constant__ int mask[N_F * N_F];\n",
        "\n",
        "__global__ void gpu_matrix_convolution_tiling(int *input_matrix, int *output_matrix, int n_m, int n_f) {\n",
        "    __shared__ int shared_input_matrix[TILE_WIDTH + N_F - 1][TILE_WIDTH + N_F - 1];\n",
        "\n",
        "    int tx = threadIdx.x;\n",
        "    int ty = threadIdx.y;\n",
        "\n",
        "    int row = blockIdx.y * TILE_WIDTH + ty;\n",
        "    int col = blockIdx.x * TILE_WIDTH + tx;\n",
        "\n",
        "    int start_r = row - n_f / 2;\n",
        "    int start_c = col - n_f / 2;\n",
        "\n",
        "    // Load input matrix tile into shared memory with boundary checks\n",
        "    if (ty < TILE_WIDTH + N_F - 1 && tx < TILE_WIDTH + N_F - 1) {\n",
        "        if (start_r >= 0 && start_r < n_m && start_c >= 0 && start_c < n_m) {\n",
        "            shared_input_matrix[ty][tx] = input_matrix[start_r * n_m + start_c];\n",
        "        } else {\n",
        "            shared_input_matrix[ty][tx] = 0;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    // Compute the convolution\n",
        "    if (ty < TILE_WIDTH && tx < TILE_WIDTH && row < n_m && col < n_m) {\n",
        "        int temp = 0;\n",
        "        #pragma unroll\n",
        "        for (int i = 0; i < n_f; i++) {\n",
        "            #pragma unroll\n",
        "            for (int j = 0; j < n_f; j++) {\n",
        "                temp += shared_input_matrix[ty + i][tx + j] * mask[i * n_f + j];\n",
        "            }\n",
        "        }\n",
        "        output_matrix[row * n_m + col] = temp;\n",
        "    }\n",
        "}\n",
        "\n",
        "void verify_result(int *m, int *mask, int *result, int n_m,int n_f) {\n",
        "  // Temp value for accumulating results\n",
        "  int temp;\n",
        "\n",
        "\n",
        "  int offset_r;\n",
        "  int offset_c;\n",
        "\n",
        "  // Go over each row\n",
        "  for (int i = 0; i < n_m; i++) {\n",
        "    // Go over each column\n",
        "    for (int j = 0; j < n_m; j++) {\n",
        "      // Reset the temp variable\n",
        "      temp = 0;\n",
        "\n",
        "      // Go over each mask row\n",
        "      for (int k = 0; k < n_f; k++) {\n",
        "        // Update offset value for row\n",
        "        offset_r = i - (n_f/2) + k;\n",
        "\n",
        "        // Go over each mask column\n",
        "        for (int l = 0; l < n_f; l++) {\n",
        "          // Update offset value for column\n",
        "          offset_c = j - (n_f/2) + l;\n",
        "\n",
        "          // Range checks if we are hanging off the matrix\n",
        "          if (offset_r >= 0 && offset_r < n_m) {\n",
        "            if (offset_c >= 0 && offset_c < n_m) {\n",
        "              // Accumulate partial results\n",
        "              temp += m[offset_r * n_m + offset_c] * mask[k * n_f + l];\n",
        "            }\n",
        "          }\n",
        "        }\n",
        "      }\n",
        "      // Fail if the results don't match\n",
        "      assert(result[i * n_m+ j] == temp);\n",
        "\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    srand(time(NULL));\n",
        "\n",
        "    int *input_matrix, *output_matrix;\n",
        "    cudaMallocManaged((void **)&input_matrix, sizeof(int) * N_M * N_M);\n",
        "    cudaMallocManaged((void **)&output_matrix, sizeof(int) * N_M * N_M);\n",
        "\n",
        "    // Initialize input matrix\n",
        "    for (int i = 0; i < N_M; ++i) {\n",
        "        for (int j = 0; j < N_M; ++j) {\n",
        "            input_matrix[i * N_M + j] = rand() % 101;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    int filter[N_F * N_F];\n",
        "    for (int i = 0; i < N_F * N_F; i++) {\n",
        "        filter[i] = rand() % 101;\n",
        "    }\n",
        "    cudaMemcpyToSymbol(mask, filter, sizeof(int) * N_F * N_F);\n",
        "    cudaMemPrefetchAsync(input_matrix, sizeof(int) * N_M * N_M, 0);\n",
        "    cudaMemPrefetchAsync(output_matrix, sizeof(int) * N_M * N_M, 0);\n",
        "\n",
        "    float gpu_elapsed_time_ms;\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "\n",
        "    dim3 dimGrid((N_M + TILE_WIDTH - 1) / TILE_WIDTH, (N_M + TILE_WIDTH - 1) / TILE_WIDTH);\n",
        "    dim3 dimBlock(TILE_WIDTH+N_F-1, TILE_WIDTH+N_F-1,1);\n",
        "\n",
        "    cudaEventRecord(start, 0);\n",
        "    gpu_matrix_convolution_tiling<<<dimGrid, dimBlock>>>(input_matrix, output_matrix, N_M, N_F);\n",
        "    cudaDeviceSynchronize();\n",
        "    cudaEventRecord(stop, 0);\n",
        "    cudaEventSynchronize(stop);\n",
        "\n",
        "    cudaEventElapsedTime(&gpu_elapsed_time_ms, start, stop);\n",
        "    printf(\"Time elapsed on GPU matrix convolution of %dx%d: %f ms.\\n\", N_M, N_M, gpu_elapsed_time_ms);\n",
        "\n",
        "    verify_result(input_matrix, filter, output_matrix, N_M, N_F);\n",
        "    printf(\"Success\\n\");\n",
        "\n",
        "    cudaFree(input_matrix);\n",
        "    cudaFree(output_matrix);\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5jAXMebnvg4",
        "outputId": "060ca2b4-45e7-4822-ab27-15960adf0116"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time elapsed on GPU matrix convolution of 16000x16000: 47.499039 ms.\n",
            "Success\n",
            "\n"
          ]
        }
      ]
    }
  ]
}